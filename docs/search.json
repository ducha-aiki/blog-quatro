[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dmytro Mishkin’s blog."
  },
  {
    "objectID": "posts/2021-02-18-how-i-read-arxiv.html",
    "href": "posts/2021-02-18-how-i-read-arxiv.html",
    "title": "How I cope with the flood of arXiv papers",
    "section": "",
    "text": "OK, that is a joke – I believe that I am dealing with the tons of papers rather fine. Here is how."
  },
  {
    "objectID": "posts/2021-02-18-how-i-read-arxiv.html#coarse-to-fine-scheme",
    "href": "posts/2021-02-18-how-i-read-arxiv.html#coarse-to-fine-scheme",
    "title": "How I cope with the flood of arXiv papers",
    "section": "Coarse-to-fine scheme",
    "text": "Coarse-to-fine scheme\nI check 100 paper titles + abstracts, I skim through maybe 10, I read carefully one or two papers. Why? The most of papers are not relevant to me as a computer vision researcher. Some of the papers are bad. From those, which are good, the most important thing is their main message, not some details. And only little number of papers are worth reading – for me. For you that would be different 1 or 2 papers out of 100, but likely not more."
  },
  {
    "objectID": "posts/2025-09-17-chatGPT-as-a-student.html",
    "href": "posts/2025-09-17-chatGPT-as-a-student.html",
    "title": "How ChatGPT fare in computer vision course homeworks at CTU in Prague.",
    "section": "",
    "text": "In the era of SWE-bench and various arenas for LLMs, the question “can LLMs code?” is obsolete. Of course, they can and I personally use ChatGPT and Sonnet4 daily. They are good.\nNow, the real question is – if the LLMs are so good, why students are struggling with coding assignments for computer vision course every year? Does Chat have any problem with coding the gaussian blur? Are students very honest and not using it at all? Are they using LLMs, but something is wrong?\nThose are the questions, which I have tried to answer last semester.\n\n\nI do the lectures and labs on the image matching part of the computer vision. During the series of 5 assignments, students have to implement:\n\nGaussian blurring and patch extraction\n(Multiscale) Harris corner detector\nSIFT descriptor and patch dominant orientation estimation\nSNN matching and homography RANSAC\nAssemble altogether into image matching pipeline.\n\nStudents have got the lectures with a theory, course wiki with task descriptions and assignment templates with API of the functions they have to implement. Most of the assignments are evaluated automatically by unit-tests and benchmarks on university server.\nIn the end, result should look like this (the widgets are provided with templates). \n\n\n\nObviously, there are many possible ways of doing it. I have selected the following: I copy-paste 3 verbatims in a single message. The structure is :\n\nPlease, implement {ASSIGNMENT-DESCRIPTION}\n{ALL-ASSIGNMENT-RELATED-PAGE}\nCode template: {TEMPLATE.PY}\n\nThis would be (almost) fair to an LLM, as it would have the same information, as the student, except the images. It also would imitate a prompt from a reasonable, but lazy student.\nAn real (shortened) example would be:\n\n\nPlease, implement imagefiltering.py - file with the following methods implemented:\ngaussian1d, gaussian_deriv1d - functions for computing Gaussian function and its first derivative.\n\n\n\n\nimage-4.png\n\n\nimport numpy as np\nimport math\nimport torch\nimport torch.nn.functional as F\nimport typing\n\n\ndef get_gausskernel_size(sigma, force_odd = True):\n    ksize = 2 * math.ceil(sigma * 3.0) + 1\n    if ksize % 2  == 0 and force_odd:\n        ksize +=1\n    return int(ksize)\n\n\ndef gaussian1d(x: torch.Tensor, sigma: float) -&gt; torch.Tensor: \n    '''Function that computes values of a (1D) Gaussian with zero mean and variance sigma^2'''\n    out =  torch.zeros(x.shape)\n    return out\n\n\ndef gaussian_deriv1d(x: torch.Tensor, sigma: float) -&gt; torch.Tensor:  \n    '''Function that computes values of a (1D) Gaussian derivative'''\n    out =  torch.zeros(x.shape)\n    return out\n\n\n\n\nDuring the course of the semester, I have prompted the 4o, o3mini, o3 mini-hight, and o1 OpenAI models the same way, saved the .py file and uploaded it to the automatic evaluation system. When the current assignment required the functions from the previous weeks, I imported the ground truth solution – assuming that the students could perfect them via multiple attempts (we allow up to ~50 attempts to reupload homework).\nHere are the results (I added GPT5 later):\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel/task\n4o\no3-mini\no3-mini-high\no1\nGPT5-Instant\nGPT5-Thinking\nMaximum\n\n\n\n\nImage filtering\n3\n5\n2\n6\n6\n7\n7\n\n\nHarris detector\n4\n5\n5\n6\n6\n6\n7\n\n\nSIFT descriptor\n2\n4\n0\n3\n4\n4\n4\n\n\nMatching and RANSAC\n3\n7\n7\n6\n8\n3\n8\n\n\nTotal points\n12\n21\n14\n21\n24\n20\n26\n\n\nPercentage [%]\n46\n81\n54\n81\n92\n77\n100\n\n\n\nOverall, LLMs did better, than an average student on their first attempt. If prompted with scores and errors, they were pretty good at fixing their mistakes. This makes me wonder, why people, who obviously used LLM for their assignment still struggle to get high scores. And those, who get the highest scores are mostly NOT using LLMs, given the style and the iterations required to get to the answer.\nFunny enough, the tasks, which students usually struggle with, gave LLMs hard times as well.\nFor example, almost everyone missed the following sentence about strict maximum in the nms2d function description.\n (i.e. take to consideration all 8 points in the 3×3 neigborhood and output non-zero only if the center is strictly larger than the neighborhood)\nTwo of the tasks are actually benchmark based – patch orientation datection, and SIFT patch descriptor. Here are the results:\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark/model\n4o\no3-mini\no3-mini-high\no1\nGPT5-Instant\nGPT5-Thinking\nReference Implementation\n\n\n\n\nAng. Error [deg]\nfail\n12.30\ntimeout\n22.10\n14.1\n4.20\n1.1\n\n\nSIFT mAP [%]\nfail\n64.8\ntimeout\nfail\n68.4\n73\n71.7\n\n\n\nOriginally I wanted to evaluate all CV tasks from our course – including retrieval, CNN training, tracking. However, dirung the semester, OpenAI has been updating and changing their models a couple of times, and I lost the motivation to redo the work.\n\n\n\nI believe, modern LLMs – even if in “single attempt” mode could easily ace university coding assignemnt. Even more so in “agentic” mode. Relying on ban for LLM usage is futile. Instead we should ask questions like - how do we teach people to do software engineering, and especially deep understanding of the algorithms in the ChatGPT era. The emphasis here on the understanding and the final result - good engineer, and not on the stupid things like “how can we prevent students from using LLMs”. We cannot.\nWhat is troubling to me, is when student could not get good results even with ChatGPT help."
  },
  {
    "objectID": "posts/2025-09-17-chatGPT-as-a-student.html#evaluation",
    "href": "posts/2025-09-17-chatGPT-as-a-student.html#evaluation",
    "title": "How ChatGPT fare in computer vision course homeworks at CTU in Prague.",
    "section": "",
    "text": "During the course of the semester, I have prompted the 4o, o3mini, o3 mini-hight, and o1 OpenAI models the same way, saved the .py file and uploaded it to the automatic evaluation system. When the current assignment required the functions from the previous weeks, I imported the ground truth solution – assuming that the students could perfect them via multiple attempts (we allow up to ~50 attempts to reupload homework).\nHere are the results (I added GPT5 later):\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel/task\n4o\no3-mini\no3-mini-high\no1\nGPT5-Instant\nGPT5-Thinking\nMaximum\n\n\n\n\nImage filtering\n3\n5\n2\n6\n6\n7\n7\n\n\nHarris detector\n4\n5\n5\n6\n6\n6\n7\n\n\nSIFT descriptor\n2\n4\n0\n3\n4\n4\n4\n\n\nMatching and RANSAC\n3\n7\n7\n6\n8\n3\n8\n\n\nTotal points\n12\n21\n14\n21\n24\n20\n26\n\n\nPercentage [%]\n46\n81\n54\n81\n92\n77\n100\n\n\n\nOverall, LLMs did better, than an average student on their first attempt. If prompted with scores and errors, they were pretty good at fixing their mistakes. This makes me wonder, why people, who obviously used LLM for their assignment still struggle to get high scores. And those, who get the highest scores are mostly NOT using LLMs, given the style and the iterations required to get to the answer.\nFunny enough, the tasks, which students usually struggle with, gave LLMs hard times as well.\nFor example, almost everyone missed the following sentence about strict maximum in the nms2d function description.\n (i.e. take to consideration all 8 points in the 3×3 neigborhood and output non-zero only if the center is strictly larger than the neighborhood)\nTwo of the tasks are actually benchmark based – patch orientation datection, and SIFT patch descriptor. Here are the results:\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark/model\n4o\no3-mini\no3-mini-high\no1\nGPT5-Instant\nGPT5-Thinking\nReference Implementation\n\n\n\n\nAng. Error [deg]\nfail\n12.30\ntimeout\n22.10\n14.1\n4.20\n1.1\n\n\nSIFT mAP [%]\nfail\n64.8\ntimeout\nfail\n68.4\n73\n71.7\n\n\n\nOriginally I wanted to evaluate all CV tasks from our course – including retrieval, CNN training, tracking. However, dirung the semester, OpenAI has been updating and changing their models a couple of times, and I lost the motivation to redo the work."
  },
  {
    "objectID": "posts/2025-09-17-chatGPT-as-a-student.html#conclusion",
    "href": "posts/2025-09-17-chatGPT-as-a-student.html#conclusion",
    "title": "How ChatGPT fare in computer vision course homeworks at CTU in Prague.",
    "section": "",
    "text": "I believe, modern LLMs – even if in “single attempt” mode could easily ace university coding assignemnt. Even more so in “agentic” mode. Relying on ban for LLM usage is futile. Instead we should ask questions like - how do we teach people to do software engineering, and especially deep understanding of the algorithms in the ChatGPT era. The emphasis here on the understanding and the final result - good engineer, and not on the stupid things like “how can we prevent students from using LLMs”. We cannot.\nWhat is troubling to me, is when student could not get good results even with ChatGPT help."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dmytro's Blog",
    "section": "",
    "text": "Beer or Trdelnik or The Tale of Data in Compiter Vision\n\n\n\n\n\n\ncomputer vision\n\n\nresearch\n\n\nteaching\n\n\nAI\n\n\n\nSome fun for the Vision and Sport Summer School\n\n\n\n\n\nDec 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow ChatGPT fare in computer vision course homeworks at CTU in Prague.\n\n\n\n\n\n\ncomputer vision\n\n\nresearch\n\n\nteaching\n\n\nAI\n\n\n\nBetter than average human?\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow I cope with the flood of arXiv papers\n\n\n\n\n\n\narXiv\n\n\nresearch\n\n\npreprints\n\n\n\nhope, that it could help you as well\n\n\n\n\n\nFeb 18, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html",
    "href": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html",
    "title": "Beer or Trdelnik or The Tale of Data in Compiter Vision",
    "section": "",
    "text": "What would a good assignment for the summer school in computer vision? That was the question I suddenly got puzzled with, because of the last minute replacement for another teacher I agreed to do for the Vision and Sports School 2022 in Prague.\nWhat would I like to get as a student? Something fun, probably related to the computer vision, but not your typical homework or coursework. Something light enough to have fun, but kind of useful. Something with a take-home message, but learned in personal experience, not written on a whiteboard.\nAnd what is the most important thing in the all machine learning? Of course, it is data. So…I have got an idea.\n\n\nOn the day of my course I have delivered some (quick) talk on the deep learning, and we proceeded to the computer class. The students got the link to the Google Colab with fastai working example of training image classification model. No coding is required, but they are free to do it if they want, e.g. to explore various CNN/ViT architectures etc. The task is the following:\nYou have to train a two-way image classification model.\nClass 1 is “Beer” – hardly can be more Czech than this. \nClass 2 is “Trdelnik” or “Chimney cake” as someone translates this into English – tourist-trap pastry, not a traditional at all, but sold everywhere in Prague. I actually like it - what can be wrong about dough + sugar, right? \nYour job is to produce the model, which is as good as possible. You may write a google search scrapper, look for the ready datasets, draw images yourself, generate synthetic data – I do not care. You can use ImageNet-pretrained models, or anything else.\nWhen you think, you are done, you call me. I came to your desk, download the test set from my super secret website, and we benchmark it together (but nobody else can see the images).\nThe leadearboard with the best score is written on the physical whiteboard in the class. The leaderboard is initalized with two entries: 0% – best result so far.\nThe person, whos model performs the best, wins.\n\n\n\nThe next hour or so, people were constructing their datasets. Most advanced ones even created their validation sets. When the accuracy on the validation set reached 98%, someone got enough confidence and called me.\nWe did a benchmark and the result was 28.5% accuracy. The class giggled a bit. Next 20 minutes nobody could cross this line. The random chance would do better than any of the models. The finally new breakthrough came - 42%. People started to realise the task was not as easy as they thought.\nNext hour people finally got 57%. The class was giggling and laughing with every trial – the number on the leaderboard was unchanging. Finally someone used CLIP model and got astounishing 71%.\n\n\n\nWhat do you usually imagine, when someone says “beer”? Maybe a nice glass of pilsner or IPA. Or maybe a bottle. Rarely a can. What do you imagine, when you hear trdelnik? Probably nothing, unless you live in Prague or visit the city. Then you see that special fire cage, where the pastry is done.\nSo, I have selected the can beer, the beer near the fire-pit for barbeque, Duff from The Simpsons, and top-down view of the beer glasses near the shashlyck for the Beer class. For the Trdelnik I selected close-up view with a toppings – not popular at that time yet – similar to the beer foam, and the kiosk selling the trdelnik. Finally, to make people less sad, I have found the image, which contains both, so any answer would be correct. Here is my test set in full:\n\n\n\nTest dataset: 7 images\n\n\nI have also create another test set with similar ideas of naturally adversarial examples – for those, who wanted to try again. It was a bit simpler, but also challenging.\n\n\n\nThis trick worked for couple more years – and then VLMs came. I have tested PaliGemma on my tiny dataset – and it gave me 85% accuracy out of the box. Time to invent something new."
  },
  {
    "objectID": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#the-task",
    "href": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#the-task",
    "title": "Beer or Trdelnik or The Tale of Data in Compiter Vision",
    "section": "",
    "text": "On the day of my course I have delivered some (quick) talk on the deep learning, and we proceeded to the computer class. The students got the link to the Google Colab with fastai working example of training image classification model. No coding is required, but they are free to do it if they want, e.g. to explore various CNN/ViT architectures etc. The task is the following:\nYou have to train a two-way image classification model.\nClass 1 is “Beer” – hardly can be more Czech than this. \nClass 2 is “Trdelnik” or “Chimney cake” as someone translates this into English – tourist-trap pastry, not a traditional at all, but sold everywhere in Prague. I actually like it - what can be wrong about dough + sugar, right? \nYour job is to produce the model, which is as good as possible. You may write a google search scrapper, look for the ready datasets, draw images yourself, generate synthetic data – I do not care. You can use ImageNet-pretrained models, or anything else.\nWhen you think, you are done, you call me. I came to your desk, download the test set from my super secret website, and we benchmark it together (but nobody else can see the images).\nThe leadearboard with the best score is written on the physical whiteboard in the class. The leaderboard is initalized with two entries: 0% – best result so far.\nThe person, whos model performs the best, wins."
  },
  {
    "objectID": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#what-is-the-catch",
    "href": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#what-is-the-catch",
    "title": "Beer or Trdelnik or The Tale of Data in Compiter Vision",
    "section": "",
    "text": "The next hour or so, people were constructing their datasets. Most advanced ones even created their validation sets. When the accuracy on the validation set reached 98%, someone got enough confidence and called me.\nWe did a benchmark and the result was 28.5% accuracy. The class giggled a bit. Next 20 minutes nobody could cross this line. The random chance would do better than any of the models. The finally new breakthrough came - 42%. People started to realise the task was not as easy as they thought.\nNext hour people finally got 57%. The class was giggling and laughing with every trial – the number on the leaderboard was unchanging. Finally someone used CLIP model and got astounishing 71%."
  },
  {
    "objectID": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#the-catch-is-the-data",
    "href": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#the-catch-is-the-data",
    "title": "Beer or Trdelnik or The Tale of Data in Compiter Vision",
    "section": "",
    "text": "What do you usually imagine, when someone says “beer”? Maybe a nice glass of pilsner or IPA. Or maybe a bottle. Rarely a can. What do you imagine, when you hear trdelnik? Probably nothing, unless you live in Prague or visit the city. Then you see that special fire cage, where the pastry is done.\nSo, I have selected the can beer, the beer near the fire-pit for barbeque, Duff from The Simpsons, and top-down view of the beer glasses near the shashlyck for the Beer class. For the Trdelnik I selected close-up view with a toppings – not popular at that time yet – similar to the beer foam, and the kiosk selling the trdelnik. Finally, to make people less sad, I have found the image, which contains both, so any answer would be correct. Here is my test set in full:\n\n\n\nTest dataset: 7 images\n\n\nI have also create another test set with similar ideas of naturally adversarial examples – for those, who wanted to try again. It was a bit simpler, but also challenging."
  },
  {
    "objectID": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#everything-good-comes-to-an-end",
    "href": "posts/2025-12-23-Beer-or-trdelnik-data-tale.html#everything-good-comes-to-an-end",
    "title": "Beer or Trdelnik or The Tale of Data in Compiter Vision",
    "section": "",
    "text": "This trick worked for couple more years – and then VLMs came. I have tested PaliGemma on my tiny dataset – and it gave me 85% accuracy out of the box. Time to invent something new."
  }
]